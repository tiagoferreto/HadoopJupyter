{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop - multi-node cluster setup \n",
    "![Hadoop](https://hadoop.apache.org/elephant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hadoop base image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker container\n",
    "\n",
    "- Ubuntu 18.04 (https://ubuntu.com/)\n",
    "- Docker (https://www.docker.com/)\n",
    "    - container based virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "docker run -d -t --rm --name hadoopimg -h hadoopimg ubuntu:18.04\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "- Java 8 (OpenJDK) - https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n",
    "- Other packages: ssh pdsh wget apt-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Update package list\n",
    "apt -qq update > /install.log 2>&1\n",
    "\n",
    "# Install Hadoop dependencies\n",
    "apt -qq -f -y install openjdk-8-jdk ssh pdsh >> /install.log 2>&1\n",
    "\n",
    "# Install other dependencies\n",
    "apt -qq -f -y install vim wget apt-utils python3 python3-pip \\\n",
    "    ipython3 less unzip sudo net-tools >> /install.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop\n",
    "\n",
    "- http://hadoop.apache.org/\n",
    "- Version 3.2.1\n",
    "- Base directory: /opt\n",
    "- User/group: hadoop/hadoop\n",
    "- Package with binaries (version 3.2.1): https://hadoop.apache.org/releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Enable rwx for all on /opt\n",
    "chmod 777 /opt\n",
    "\n",
    "# Create user/group hadoop\n",
    "useradd -m -U -s /bin/bash hadoop\n",
    "\n",
    "# Enable sudo for hadoop\n",
    "sed -i \"\\$ahadoop  ALL=(ALL) NOPASSWD:ALL\" /etc/sudoers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd ../pkgs\n",
    "wget -q -c https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp hadoop-3.2.1.tar.gz hadoopimg:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Modify user/group permissions and unpack file\n",
    "sudo chown hadoop:hadoop /opt/hadoop-3.2.1.tar.gz\n",
    "tar -zxf /opt/hadoop-3.2.1.tar.gz -C /opt\n",
    "rm /opt/hadoop-3.2.1.tar.gz\n",
    "\n",
    "# Create link\n",
    "ln -s /opt/hadoop-3.2.1 /opt/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables\n",
    "\n",
    "- Create file /opt/envvars.sh with environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/envvars.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export HADOOP_COMMON_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=\\${HADOOP_HOME}/etc/hadoop\n",
    "export HADOOP_HDFS_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_MAPRED_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=\\${HADOOP_HOME}\n",
    "\n",
    "export PATH=\\${PATH}:\\${HADOOP_HOME}/bin:\\${HADOOP_HOME}/sbin     \n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure passwordless ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Disable host key checking\n",
    "sudo tee -a /etc/ssh/ssh_config << EOF\n",
    "    StrictHostKeyChecking no\n",
    "    UserKnownHostsFile /dev/null\n",
    "EOF\n",
    "\n",
    "# Create ssh key\n",
    "ssh-keygen -q -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "# Copy public key to authorized_keys file\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop configuration files\n",
    "\n",
    "- Hadoop configuration files location: \\$HADOOP\\_HOME\\/etc\\/hadoop\n",
    "- All cluster nodes contain the same files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hadoop-env.sh\n",
    "\n",
    "- Definition of environment variables used by Hadoop processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat >> /opt/hadoop/etc/hadoop/hadoop-env.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core-site.xml\n",
    "\n",
    "- Hadoop main configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/core-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/core-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hdfs-site.xml\n",
    "\n",
    "- HDFS configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/hadoop/data/nameNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/hadoop/data/dataNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>33554432</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.hosts.exclude</name>\n",
    "    <value>/opt/hadoop/etc/hadoop/dfs.exclude</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.heartbeat.recheck-interval</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yarn-site.xml\n",
    "\n",
    "- YARN configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/yarn-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.system-metrics-publisher.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.log-aggregation-enable</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nm.liveness-monitor.expiry-interval-ms</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapred-site.xml\n",
    "\n",
    "- MapReduce configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/mapred-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/mapreduce/lib/*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workers\n",
    "\n",
    "- List of worker nodes (NodeManager and DataNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/workers << EOF\n",
    "hadoop1\n",
    "hadoop2\n",
    "hadoop3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create hadoopimg image based on hadoop container\n",
    "docker commit hadoopimg hadoopimg\n",
    "\n",
    "# Stop base container\n",
    "docker stop hadoopimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# MASTER\n",
    "\n",
    "# Ports\n",
    "# 9870 - Namenode\n",
    "# 9868 - Secondary Namenode\n",
    "# 8088 - ResourceManager\n",
    "# 19888 - MapReduce Job History\n",
    "# 8188 - Timeline Service\n",
    "# 4040 - Spark Application UI\n",
    "# 8080 - Jupyter\n",
    "\n",
    "cd ..\n",
    "docker run -d -t --memory 4g --memory-swap 4g --rm --name hadoop -h hadoop -u hadoop \\\n",
    "    -v \"$(pwd)\"/pkgs:/opt/pkgs -v \"$(pwd)\"/notebooks:/opt/notebooks -v \"$(pwd)\"/datasets:/opt/datasets \\\n",
    "    -p 9870:9870 -p 9868:9868 -p 8088:8088 -p 19888:19888 -p 8188:8188 -p 4040:4040 -p 8080:8080 hadoopimg\n",
    "\n",
    "# WORKERS\n",
    "\n",
    "# Ports\n",
    "# 9864 - DataNode WebUI\n",
    "# 8042 - NodeManager WebUI\n",
    "\n",
    "# Hadoop1\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop1 -h hadoop1 -u hadoop \\\n",
    "    -p 9864:9864 -p 8042:8042 hadoopimg\n",
    "# Hadoop2\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop2 -h hadoop2 -u hadoop \\\n",
    "    -p 9865:9864 -p 8043:8042  hadoopimg\n",
    "# Hadoop3\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop3 -h hadoop3 -u hadoop \\\n",
    "    -p 9866:9864 -p 8044:8042  hadoopimg\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure hosts file on all nodes\n",
    "\n",
    "- /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Get IPs\n",
    "M=$(docker inspect hadoop | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H1=$(docker inspect hadoop1 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H2=$(docker inspect hadoop2 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H3=$(docker inspect hadoop3 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "\n",
    "# Create hosts file\n",
    "cat > hosts << EOF  \n",
    "$M hadoop\n",
    "$H1 hadoop1\n",
    "$H2 hadoop2\n",
    "$H3 hadoop3\n",
    "EOF\n",
    "\n",
    "cat hosts\n",
    "\n",
    "# Copy to all nodes\n",
    "docker exec -i -u root hadoop sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop1 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop2 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop3 sh -c 'cat >> /etc/hosts' < hosts\n",
    "\n",
    "# Remove local file\n",
    "rm hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ssh server on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3\n",
    "do\n",
    "    echo $HOST\n",
    "    docker exec -u root $HOST service ssh restart\n",
    "    docker exec -u root $HOST service ssh status\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format HDFS on Namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop daemons\n",
    "\n",
    "- manual execution: ```hdfs --daemon start (namenode|datanode)``` and ```yarn --daemon start (resourcemanager|nodemanager)```\n",
    "- auxilliary scripts to run all processes on the cluster: start-dfs.sh (HDFS) and start-yarn.sh (YARN)\n",
    "- some services still need to be executed manually (timelineserver, historyserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# HDFS\n",
    "start-dfs.sh\n",
    "\n",
    "# YARN\n",
    "start-yarn.sh\n",
    "\n",
    "# timelineserver\n",
    "yarn --daemon start timelineserver\n",
    "\n",
    "# historyserver\n",
    "mapred --daemon start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Listing all processes\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    echo $HOST\n",
    "    docker exec $HOST jps\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDFS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hadoop\n",
    "hdfs dfs -chown hadoop:hadoop /user/hadoop\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -chmod 777 /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Jupyter on master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# pip3 -q install notebook\n",
    "pip3 -q install jupyterlab\n",
    "\n",
    "IP=$(ifconfig eth0 | grep inet | awk '{ print $2 }')\n",
    "\n",
    "cd /opt\n",
    "\n",
    "export SHELL=/bin/bash\n",
    "# nohup /home/hadoop/.local/bin/jupyter-notebook --ip=$IP --port=8080 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/' --no-browser &\n",
    "nohup /home/hadoop/.local/bin/jupyter-lab --ip=$IP --port=8080 --notebook-dir='/' --ServerApp.token='' --ServerApp.password='' --no-browser &\n",
    "\n",
    "echo $! > /tmp/jupyter.pid\n",
    "\n",
    "# To kill\n",
    "# kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access web interfaces\n",
    "\n",
    "- Jupyterlab\n",
    "    - http://localhost:8080\n",
    "- Master - hadoop\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "    - MapReduce Job History: http://localhost:19888\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Workers\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run mapreduce Pi example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar pi 6 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUTDOWN PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Hadoop daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "yarn --daemon stop timelineserver\n",
    "mapred --daemon stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    docker stop $HOST\n",
    "done\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

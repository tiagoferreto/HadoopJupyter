{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop - multi-node cluster setup \n",
    "![Hadoop](https://hadoop.apache.org/elephant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hadoop base image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker container\n",
    "\n",
    "- Ubuntu 18.04 (https://ubuntu.com/)\n",
    "- Docker (https://www.docker.com/)\n",
    "    - container based virtualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find image 'ubuntu:18.04' locally\n",
      "18.04: Pulling from library/ubuntu\n",
      "13f679bb266a: Pulling fs layer\n",
      "13f679bb266a: Verifying Checksum\n",
      "13f679bb266a: Download complete\n",
      "13f679bb266a: Pull complete\n",
      "Digest: sha256:8aa9c2798215f99544d1ce7439ea9c3a6dfd82de607da1cec3a8a2fae005931b\n",
      "Status: Downloaded newer image for ubuntu:18.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0587614fdd18894f9ede098f32ba377ce4e7e36ee51438c3e02a2165284741d\n",
      "CONTAINER ID   IMAGE          COMMAND       CREATED                  STATUS                  PORTS     NAMES\n",
      "a0587614fdd1   ubuntu:18.04   \"/bin/bash\"   Less than a second ago   Up Less than a second             hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "docker run -d -t --rm --name hadoopimg -h hadoopimg ubuntu:18.04\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "- Java 8 (OpenJDK) - https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n",
    "- Other packages: ssh pdsh wget apt-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Update package list\n",
    "apt -qq update > /install.log 2>&1\n",
    "\n",
    "# Install Hadoop dependencies\n",
    "apt -qq -f -y install openjdk-8-jdk ssh pdsh >> /install.log 2>&1\n",
    "\n",
    "# Install other dependencies\n",
    "apt -qq -f -y install vim wget apt-utils python3 python3-pip \\\n",
    "    ipython3 less unzip sudo net-tools >> /install.log 2>&1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Hadoop\n",
    "\n",
    "- http://hadoop.apache.org/\n",
    "- Version 3.2.4\n",
    "- Base directory: /opt\n",
    "- User/group: hadoop/hadoop\n",
    "- Package with binaries (version 3.2.4): https://hadoop.apache.org/releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoopimg\n",
    "\n",
    "# Enable rwx for all on /opt\n",
    "chmod 777 /opt\n",
    "\n",
    "# Create user/group hadoop\n",
    "useradd -m -U -s /bin/bash hadoop\n",
    "\n",
    "# Enable sudo for hadoop\n",
    "sed -i \"\\$ahadoop  ALL=(ALL) NOPASSWD:ALL\" /etc/sudoers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HADOOPVERSION=hadoop-3.2.4\n",
    "\n",
    "# Download package\n",
    "cd ../pkgs\n",
    "wget -q -c https://dlcdn.apache.org/hadoop/common/$HADOOPVERSION/$HADOOPVERSION.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp $HADOOPVERSION.tar.gz hadoopimg:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "HADOOPVERSION=hadoop-3.2.4\n",
    "\n",
    "# Modify user/group permissions and unpack file\n",
    "sudo chown hadoop:hadoop /opt/$HADOOPVERSION.tar.gz\n",
    "tar -zxf /opt/$HADOOPVERSION.tar.gz -C /opt\n",
    "rm /opt/$HADOOPVERSION.tar.gz\n",
    "\n",
    "# Create link\n",
    "ln -s /opt/$HADOOPVERSION /opt/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables\n",
    "\n",
    "- Create file /opt/envvars.sh with environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/envvars.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "export HADOOP_COMMON_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_CONF_DIR=\\${HADOOP_HOME}/etc/hadoop\n",
    "export HADOOP_HDFS_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_MAPRED_HOME=\\${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=\\${HADOOP_HOME}\n",
    "\n",
    "export PATH=\\${PATH}:\\${HADOOP_HOME}/bin:\\${HADOOP_HOME}/sbin     \n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure passwordless ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    StrictHostKeyChecking no\n",
      "    UserKnownHostsFile /dev/null\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "# Disable host key checking\n",
    "sudo tee -a /etc/ssh/ssh_config << EOF\n",
    "    StrictHostKeyChecking no\n",
    "    UserKnownHostsFile /dev/null\n",
    "EOF\n",
    "\n",
    "# Create ssh key\n",
    "ssh-keygen -q -t rsa -P \"\" -f ~/.ssh/id_rsa\n",
    "\n",
    "# Copy public key to authorized_keys file\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop configuration files\n",
    "\n",
    "- Hadoop configuration files location: \\$HADOOP\\_HOME\\/etc\\/hadoop\n",
    "- All cluster nodes contain the same files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hadoop-env.sh\n",
    "\n",
    "- Definition of environment variables used by Hadoop processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat >> /opt/hadoop/etc/hadoop/hadoop-env.sh << EOF\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### core-site.xml\n",
    "\n",
    "- Hadoop main configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/core-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/core-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hdfs-site.xml\n",
    "\n",
    "- HDFS configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/hadoop/data/nameNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/hadoop/data/dataNode</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>33554432</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.hosts.exclude</name>\n",
    "    <value>/opt/hadoop/etc/hadoop/dfs.exclude</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.namenode.heartbeat.recheck-interval</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### yarn-site.xml\n",
    "\n",
    "- YARN configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/yarn-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.timeline-service.hostname</name>\n",
    "    <value>hadoop</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.system-metrics-publisher.enabled</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.log-aggregation-enable</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.nm.liveness-monitor.expiry-interval-ms</name>\n",
    "    <value>10000</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapred-site.xml\n",
    "\n",
    "- MapReduce configuration\n",
    "- Default parameters: http://hadoop.apache.org/docs/r3.2.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/mapred-site.xml << EOF\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.application.classpath</name>\n",
    "    <value>/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/mapreduce/lib/*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workers\n",
    "\n",
    "- List of worker nodes (NodeManager and DataNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoopimg\n",
    "\n",
    "cat > /opt/hadoop/etc/hadoop/workers << EOF\n",
    "hadoop1\n",
    "hadoop2\n",
    "hadoop3\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:1b09822962966bb2addff9aa8938be8f3883e084e063c16c5e61df105b0acf18\n",
      "hadoopimg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create hadoopimg image based on hadoop container\n",
    "docker commit hadoopimg hadoopimg\n",
    "\n",
    "# Stop base container\n",
    "docker stop hadoopimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86319dfbc48faee5578f7830a8d09232b9262daab530401b5aae5820a915b070\n",
      "c9eb315ea92a21fd4a0abea6bd0c08ee3897165e9cfe96dddfccf1a876e1038d\n",
      "ed14fd88984e3bce616dcb0e7944087d7e72bd0119e3195902d6db02b4a80bd7\n",
      "26dcbc50eb9f50ed97b5c917687124a77c2f603e5456eba615a3fbcc06108406\n",
      "CONTAINER ID   IMAGE       COMMAND       CREATED                  STATUS                  PORTS                                                                                                                                                                      NAMES\n",
      "26dcbc50eb9f   hadoopimg   \"/bin/bash\"   Less than a second ago   Up Less than a second   0.0.0.0:8044->8042/tcp, 0.0.0.0:9866->9864/tcp                                                                                                                             hadoop3\n",
      "ed14fd88984e   hadoopimg   \"/bin/bash\"   Less than a second ago   Up Less than a second   0.0.0.0:8043->8042/tcp, 0.0.0.0:9865->9864/tcp                                                                                                                             hadoop2\n",
      "c9eb315ea92a   hadoopimg   \"/bin/bash\"   1 second ago             Up Less than a second   0.0.0.0:8042->8042/tcp, 0.0.0.0:9864->9864/tcp                                                                                                                             hadoop1\n",
      "86319dfbc48f   hadoopimg   \"/bin/bash\"   2 seconds ago            Up Less than a second   0.0.0.0:4040->4040/tcp, 0.0.0.0:8080->8080/tcp, 0.0.0.0:8088->8088/tcp, 0.0.0.0:8188->8188/tcp, 0.0.0.0:9868->9868/tcp, 0.0.0.0:9870->9870/tcp, 0.0.0.0:19888->19888/tcp   hadoop\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# MASTER\n",
    "\n",
    "# Ports\n",
    "# 9870 - Namenode\n",
    "# 9868 - Secondary Namenode\n",
    "# 8088 - ResourceManager\n",
    "# 19888 - MapReduce Job History\n",
    "# 8188 - Timeline Service\n",
    "# 4040 - Spark Application UI\n",
    "# 8080 - Jupyter\n",
    "\n",
    "cd ..\n",
    "docker run -d -t --memory 4g --memory-swap 4g --rm --name hadoop -h hadoop -u hadoop \\\n",
    "    -v \"$(pwd)\"/pkgs:/opt/pkgs -v \"$(pwd)\"/notebooks:/opt/notebooks -v \"$(pwd)\"/datasets:/opt/datasets \\\n",
    "    -p 9870:9870 -p 9868:9868 -p 8088:8088 -p 19888:19888 -p 8188:8188 -p 4040:4040 -p 8080:8080 hadoopimg\n",
    "\n",
    "# WORKERS\n",
    "\n",
    "# Ports\n",
    "# 9864 - DataNode WebUI\n",
    "# 8042 - NodeManager WebUI\n",
    "\n",
    "# Hadoop1\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop1 -h hadoop1 -u hadoop \\\n",
    "    -p 9864:9864 -p 8042:8042 hadoopimg\n",
    "# Hadoop2\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop2 -h hadoop2 -u hadoop \\\n",
    "    -p 9865:9864 -p 8043:8042  hadoopimg\n",
    "# Hadoop3\n",
    "docker run -d -t --memory 2g --memory-swap 2g --rm --name hadoop3 -h hadoop3 -u hadoop \\\n",
    "    -p 9866:9864 -p 8044:8042  hadoopimg\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure hosts file on all nodes\n",
    "\n",
    "- /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.2 hadoop\n",
      "172.17.0.3 hadoop1\n",
      "172.17.0.4 hadoop2\n",
      "172.17.0.5 hadoop3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Get IPs\n",
    "M=$(docker inspect hadoop | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H1=$(docker inspect hadoop1 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H2=$(docker inspect hadoop2 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "H3=$(docker inspect hadoop3 | grep \\\"IPAddress\\\" | head -1 | awk '{ print $2 }' | tr -d \\\",)\n",
    "\n",
    "# Create hosts file\n",
    "cat > hosts << EOF  \n",
    "$M hadoop\n",
    "$H1 hadoop1\n",
    "$H2 hadoop2\n",
    "$H3 hadoop3\n",
    "EOF\n",
    "\n",
    "cat hosts\n",
    "\n",
    "# Copy to all nodes\n",
    "docker exec -i -u root hadoop sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop1 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop2 sh -c 'cat >> /etc/hosts' < hosts\n",
    "docker exec -i -u root hadoop3 sh -c 'cat >> /etc/hosts' < hosts\n",
    "\n",
    "# Remove local file\n",
    "rm hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ssh server on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop1\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop2\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n",
      "hadoop3\n",
      " * Restarting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n",
      " * sshd is running\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3\n",
    "do\n",
    "    echo $HOST\n",
    "    docker exec -u root $HOST service ssh restart\n",
    "    docker exec -u root $HOST service ssh status\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format HDFS on Namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /opt/hadoop/logs does not exist. Creating.\n",
      "2023-03-27 13:38:14,472 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = hadoop/172.17.0.2\n",
      "STARTUP_MSG:   args = [-format, -force, -nonInteractive]\n",
      "STARTUP_MSG:   version = 3.2.4\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.35.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.35.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.18.3.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.10.5.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-2.4.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.2.4.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.2.4.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.4.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.4-tests.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.18.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.4.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.4.jar\n",
      "STARTUP_MSG:   build = Unknown -r 7e5d9983b388e372fe640f21f048f2f2ae6e9eba; compiled by 'ubuntu' on 2022-07-12T11:58Z\n",
      "STARTUP_MSG:   java = 1.8.0_362\n",
      "************************************************************/\n",
      "2023-03-27 13:38:14,476 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2023-03-27 13:38:14,514 INFO namenode.NameNode: createNameNode [-format, -force, -nonInteractive]\n",
      "2023-03-27 13:38:14,565 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-03-27 13:38:14,706 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "2023-03-27 13:38:14,707 INFO common.Util: Assuming 'file' scheme for path /opt/hadoop/data/nameNode in configuration.\n",
      "Formatting using clusterid: CID-5fe0f326-d5a0-4440-b06d-3094b7b363ac\n",
      "2023-03-27 13:38:14,722 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2023-03-27 13:38:14,734 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2023-03-27 13:38:14,734 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2023-03-27 13:38:14,735 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2023-03-27 13:38:14,739 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)\n",
      "2023-03-27 13:38:14,739 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2023-03-27 13:38:14,739 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "2023-03-27 13:38:14,739 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2023-03-27 13:38:14,760 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2023-03-27 13:38:14,762 ERROR blockmanagement.DatanodeManager: error reading hosts files: \n",
      "java.io.FileNotFoundException: /opt/hadoop/etc/hadoop/dfs.exclude (No such file or directory)\n",
      "\tat java.io.FileInputStream.open0(Native Method)\n",
      "\tat java.io.FileInputStream.open(FileInputStream.java:195)\n",
      "\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\n",
      "\tat org.apache.hadoop.util.HostsFileReader.readFileToSet(HostsFileReader.java:79)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.readFile(HostFileManager.java:79)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:158)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager.refresh(HostFileManager.java:71)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:262)\n",
      "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:457)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:831)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:767)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1209)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1680)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1790)\n",
      "2023-03-27 13:38:14,766 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2023-03-27 13:38:14,766 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2023-03-27 13:38:14,768 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2023-03-27 13:38:14,768 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Mar 27 13:38:14\n",
      "2023-03-27 13:38:14,769 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2023-03-27 13:38:14,769 INFO util.GSet: VM type       = 64-bit\n",
      "2023-03-27 13:38:14,769 INFO util.GSet: 2.0% max memory 1.7 GB = 35.3 MB\n",
      "2023-03-27 13:38:14,769 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2023-03-27 13:38:14,779 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2023-03-27 13:38:14,779 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2023-03-27 13:38:14,782 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2023-03-27 13:38:14,782 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2023-03-27 13:38:14,782 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2023-03-27 13:38:14,782 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2023-03-27 13:38:14,782 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2023-03-27 13:38:14,783 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2023-03-27 13:38:14,783 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2023-03-27 13:38:14,783 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2023-03-27 13:38:14,783 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2023-03-27 13:38:14,783 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2023-03-27 13:38:14,795 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2023-03-27 13:38:14,795 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2023-03-27 13:38:14,795 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2023-03-27 13:38:14,795 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2023-03-27 13:38:14,802 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2023-03-27 13:38:14,802 INFO util.GSet: VM type       = 64-bit\n",
      "2023-03-27 13:38:14,802 INFO util.GSet: 1.0% max memory 1.7 GB = 17.7 MB\n",
      "2023-03-27 13:38:14,802 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2023-03-27 13:38:14,822 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2023-03-27 13:38:14,822 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2023-03-27 13:38:14,822 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2023-03-27 13:38:14,822 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2023-03-27 13:38:14,825 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2023-03-27 13:38:14,826 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2023-03-27 13:38:14,828 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2023-03-27 13:38:14,828 INFO util.GSet: VM type       = 64-bit\n",
      "2023-03-27 13:38:14,828 INFO util.GSet: 0.25% max memory 1.7 GB = 4.4 MB\n",
      "2023-03-27 13:38:14,828 INFO util.GSet: capacity      = 2^19 = 524288 entries\n",
      "2023-03-27 13:38:14,831 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2023-03-27 13:38:14,831 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2023-03-27 13:38:14,831 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2023-03-27 13:38:14,833 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2023-03-27 13:38:14,833 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2023-03-27 13:38:14,834 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2023-03-27 13:38:14,834 INFO util.GSet: VM type       = 64-bit\n",
      "2023-03-27 13:38:14,834 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 543.0 KB\n",
      "2023-03-27 13:38:14,834 INFO util.GSet: capacity      = 2^16 = 65536 entries\n",
      "2023-03-27 13:38:14,845 INFO namenode.FSImage: Allocated new BlockPoolId: BP-57750028-172.17.0.2-1679924294841\n",
      "2023-03-27 13:38:14,857 INFO common.Storage: Storage directory /opt/hadoop-3.2.4/data/nameNode has been successfully formatted.\n",
      "2023-03-27 13:38:14,875 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop-3.2.4/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2023-03-27 13:38:14,912 INFO namenode.FSImageFormatProtobuf: Image file /opt/hadoop-3.2.4/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .\n",
      "2023-03-27 13:38:14,919 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2023-03-27 13:38:14,928 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2023-03-27 13:38:14,928 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2023-03-27 13:38:14,930 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2023-03-27 13:38:14,930 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at hadoop/172.17.0.2\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs namenode -format -force -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop daemons\n",
    "\n",
    "- manual execution: ```hdfs --daemon start (namenode|datanode)``` and ```yarn --daemon start (resourcemanager|nodemanager)```\n",
    "- auxilliary scripts to run all processes on the cluster: start-dfs.sh (HDFS) and start-yarn.sh (YARN)\n",
    "- some services still need to be executed manually (timelineserver, historyserver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "hadoop: namenode is running as process 341.  Stop it first and ensure /tmp/hadoop-hadoop-namenode.pid file is empty before retry.\n",
      "pdsh@hadoop: hadoop: ssh exited with exit code 1\n",
      "Starting datanodes\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: WARNING: /opt/hadoop-3.2.4/logs does not exist. Creating.\n",
      "hadoop3: WARNING: /opt/hadoop-3.2.4/logs does not exist. Creating.\n",
      "hadoop1: WARNING: /opt/hadoop-3.2.4/logs does not exist. Creating.\n",
      "Starting secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "hadoop: secondarynamenode is running as process 580.  Stop it first and ensure /tmp/hadoop-hadoop-secondarynamenode.pid file is empty before retry.\n",
      "pdsh@hadoop: hadoop: ssh exited with exit code 1\n",
      "2023-03-27 13:41:30,804 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 822.  Stop it first and ensure /tmp/hadoop-hadoop-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "ERROR: Cannot set priority of timelineserver process 2011\n",
      "historyserver is running as process 1255.  Stop it first and ensure /tmp/hadoop-hadoop-historyserver.pid file is empty before retry.\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# HDFS\n",
    "start-dfs.sh\n",
    "\n",
    "# YARN\n",
    "start-yarn.sh\n",
    "\n",
    "# timelineserver\n",
    "yarn --daemon start timelineserver\n",
    "\n",
    "# historyserver\n",
    "mapred --daemon start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "2082 Jps\n",
      "580 SecondaryNameNode\n",
      "341 NameNode\n",
      "822 ResourceManager\n",
      "1255 JobHistoryServer\n",
      "hadoop1\n",
      "418 Jps\n",
      "293 NodeManager\n",
      "172 DataNode\n",
      "hadoop2\n",
      "163 DataNode\n",
      "409 Jps\n",
      "284 NodeManager\n",
      "hadoop3\n",
      "165 DataNode\n",
      "411 Jps\n",
      "286 NodeManager\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Listing all processes\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    echo $HOST\n",
    "    docker exec $HOST jps\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDFS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-27 13:41:51,667 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-03-27 13:41:52,264 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-03-27 13:41:52,849 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `/tmp': File exists\n",
      "2023-03-27 13:41:53,421 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hadoop\n",
    "hdfs dfs -chown hadoop:hadoop /user/hadoop\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -chmod 777 /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Jupyter on master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "[I 2023-03-27 13:44:35.441 ServerApp] jupyterlab | extension was successfully linked.\n",
      "[I 2023-03-27 13:44:35.445 ServerApp] Writing Jupyter server cookie secret to /home/hadoop/.local/share/jupyter/runtime/jupyter_cookie_secret\n",
      "[I 2023-03-27 13:44:35.538 ServerApp] nbclassic | extension was successfully linked.\n",
      "[W 2023-03-27 13:44:35.546 ServerApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n",
      "[I 2023-03-27 13:44:35.549 ServerApp] nbclassic | extension was successfully loaded.\n",
      "[I 2023-03-27 13:44:35.549 LabApp] JupyterLab extension loaded from /home/hadoop/.local/lib/python3.6/site-packages/jupyterlab\n",
      "[I 2023-03-27 13:44:35.549 LabApp] JupyterLab application directory is /home/hadoop/.local/share/jupyter/lab\n",
      "[I 2023-03-27 13:44:35.551 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "[I 2023-03-27 13:44:35.551 ServerApp] Serving notebooks from local directory: /\n",
      "[I 2023-03-27 13:44:35.551 ServerApp] Jupyter Server 1.13.1 is running at:\n",
      "[I 2023-03-27 13:44:35.551 ServerApp] http://172.17.0.2:8080/lab\n",
      "[I 2023-03-27 13:44:35.551 ServerApp]  or http://127.0.0.1:8080/lab\n",
      "[I 2023-03-27 13:44:35.551 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "# pip3 -q install notebook\n",
    "pip3 -q install pip\n",
    "pip3 -q install jupyterlab\n",
    "\n",
    "IP=$(ifconfig eth0 | grep inet | awk '{ print $2 }')\n",
    "\n",
    "cd /opt\n",
    "\n",
    "export SHELL=/bin/bash\n",
    "# nohup /home/hadoop/.local/bin/jupyter-notebook --ip=$IP --port=8080 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/' --no-browser &\n",
    "nohup /home/hadoop/.local/bin/jupyter-lab --ip=$IP --port=8080 --notebook-dir='/' --ServerApp.token='' --ServerApp.password='' --no-browser &\n",
    "\n",
    "echo $! > /tmp/jupyter.pid\n",
    "\n",
    "# To kill\n",
    "# kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access web interfaces\n",
    "\n",
    "- Jupyterlab\n",
    "    - http://localhost:8080\n",
    "- Master - hadoop\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "    - MapReduce Job History: http://localhost:19888\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Workers\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run mapreduce Pi example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 6\n",
      "Samples per Map = 10000\n",
      "2023-03-27 13:48:10,875 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Starting Job\n",
      "2023-03-27 13:48:11,787 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2023-03-27 13:48:11,866 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2023-03-27 13:48:11,916 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1679924314464_0002\n",
      "2023-03-27 13:48:11,967 INFO input.FileInputFormat: Total input files to process : 6\n",
      "2023-03-27 13:48:11,995 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2023-03-27 13:48:12,047 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1679924314464_0002\n",
      "2023-03-27 13:48:12,048 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-03-27 13:48:12,116 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-03-27 13:48:12,116 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-03-27 13:48:12,139 INFO impl.YarnClientImpl: Submitted application application_1679924314464_0002\n",
      "2023-03-27 13:48:12,153 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1679924314464_0002/\n",
      "2023-03-27 13:48:12,153 INFO mapreduce.Job: Running job: job_1679924314464_0002\n",
      "2023-03-27 13:48:15,198 INFO mapreduce.Job: Job job_1679924314464_0002 running in uber mode : false\n",
      "2023-03-27 13:48:15,200 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-03-27 13:48:19,250 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2023-03-27 13:48:20,255 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-03-27 13:48:22,267 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-03-27 13:48:22,294 INFO mapreduce.Job: Job job_1679924314464_0002 completed successfully\n",
      "2023-03-27 13:48:22,355 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=138\n",
      "\t\tFILE: Number of bytes written=1670003\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1578\n",
      "\t\tHDFS: Number of bytes written=215\n",
      "\t\tHDFS: Number of read operations=29\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32672\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1726\n",
      "\t\tTotal time spent by all map tasks (ms)=16336\n",
      "\t\tTotal time spent by all reduce tasks (ms)=863\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16336\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=863\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4182016\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=220928\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=108\n",
      "\t\tMap output materialized bytes=168\n",
      "\t\tInput split bytes=870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=168\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=402\n",
      "\t\tCPU time spent (ms)=1330\n",
      "\t\tPhysical memory (bytes) snapshot=1823789056\n",
      "\t\tVirtual memory (bytes) snapshot=12390723584\n",
      "\t\tTotal committed heap usage (bytes)=1374683136\n",
      "\t\tPeak Map Physical memory (bytes)=279597056\n",
      "\t\tPeak Map Virtual memory (bytes)=1768976384\n",
      "\t\tPeak Reduce Physical memory (bytes)=172855296\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1780064256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=708\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=97\n",
      "Job Finished in 10.678 seconds\n",
      "Estimated value of Pi is 3.14200000000000000000\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.4.jar pi 6 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUTDOWN PROCEDURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "kill $(cat /tmp/jupyter.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Hadoop daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "Stopping datanodes\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "Stopping secondary namenodes [hadoop]\n",
      "hadoop: Warning: Permanently added 'hadoop,172.17.0.2' (ECDSA) to the list of known hosts.\n",
      "2023-03-27 13:53:53,388 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping nodemanagers\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n",
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "Stopping resourcemanager\n",
      "WARNING: resourcemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec hadoop\n",
    "\n",
    "source /opt/envvars.sh\n",
    "\n",
    "stop-dfs.sh\n",
    "stop-yarn.sh\n",
    "yarn --daemon stop timelineserver\n",
    "mapred --daemon stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "hadoop1\n",
      "hadoop2\n",
      "hadoop3\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for HOST in hadoop hadoop1 hadoop2 hadoop3; do\n",
    "    docker stop $HOST\n",
    "done\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

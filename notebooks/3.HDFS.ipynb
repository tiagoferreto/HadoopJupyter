{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /opt/envvars.sh\n",
    "\n",
    "!pip3 install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "\n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# create gutenberg folder in HDFS\n",
    "# hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "\n",
    "# copy books to HDFS\n",
    "# hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg\n",
    "\n",
    "# list files in HDFS\n",
    "# hdfs dfs -ls /user/hadoop/gutenberg\n",
    "\n",
    "# show first KB of file\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "\n",
    "# show last KB of file\n",
    "# hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt\n",
    "\n",
    "# show whole file - CAREFUL\n",
    "# hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt\n",
    "\n",
    "# append file contents to a file in HDFS\n",
    "# hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt\n",
    "\n",
    "# copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n",
    "# hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "# hdfs dfs -ls -h -R\n",
    "\n",
    "# retrieve allbooks.txt from HDFS\n",
    "# hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "\n",
    "# remove file\n",
    "# hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n",
    "\n",
    "# mv file (also used for renaming)\n",
    "# hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt\n",
    "\n",
    "# print statistics on folder\n",
    "# printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "# hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*\n",
    "\n",
    "# getmerge\n",
    "# hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "\n",
    "# remove directory and files (-R recursive)\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# show head\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# remove folder on HDFS\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor of all files in directory to 3\n",
    "#hdfs dfs -setrep 3 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor back to 2\n",
    "#hdfs dfs -setrep 2 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Recomission all nodes\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-datanode.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/hdfs --daemon start datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
